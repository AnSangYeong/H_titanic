# -*- coding: utf-8 -*-
"""와인품질 회귀분석..._20220529_error_rate035.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pNa53WBPsO0TVFitYKmXRgW0vdU25wzq
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from matplotlib.pylab import rcParams
rcParams['figure.figsize'] = 18,9
rcParams['font.family'] = 'AppleGothic' #malgun Gothic


from google.colab import files     #colab 돌릴때
uploaded = files.upload()          #colab 돌릴때

red = pd.read_csv("winequality-red.csv", sep=';')             # 구분자가 콤마, 가 아닌....  세미콜론;임
print(red.shape)
red.head()

import glob      # 묶어주는 라이브러리
import os        # 폴더를 컨트롤

all_files = glob.glob(os.path.join('wine*'))
all_files

all_data = []     # 모든 데이터를 저장하기 위해 빈리스트를 만들어둠

i=0

for file in all_files:
    data = pd.read_csv(file, sep=';')
    data['type']=i
    all_data.append(data)
    i+=1         # 타입을 확인함.   레드는 0, 화이트는 1로 정의하여 와인 구분함.
    
wine = pd.concat(all_data, ignore_index=True)      # 데이터를 합침concat, concat은 axis가 기본값이   0임,  
                                                    #기존인데스를 가지고 와서 6천여개중 4900여개로 표시됨, 그래서 기존 인데스 무시처리

wine

wine.describe()

"""> **우리의 목표: 와인성분데이터를 넣으면 와인의 품질을 예측하는 모형"""

sorted(wine.quality.unique())             # 목표 변수

wine.quality.value_counts()               # 값 별 카운트

wine[wine.quality ==9]                   # 특정 값을 지는 리스트를 보고 싶어

"""### 탐색적분석  ------------------------
- 레드와인인지 화이트와인인지에 따라 품질이 다를까?
"""

wine.groupby('type').quality.describe()

red_quality = wine.loc[wine.type == 0, 'quality']

white_quality = wine.loc[wine.type==1, 'quality']

sns.distplot(red_quality, label='Red')
sns.distplot(white_quality, label="White")

"""### T검정
- 와인종류에 따른 품질의 차이가 통계적으로 유의한지 확인하기 위함
- p value < 0.05     그래야 유의미한 의미임 95%  ******
"""

import statsmodels.api as sm

tstat, pvalue, df = sm.stats.ttest_ind(red_quality, white_quality)

print("T값은 %.5f, p-value는 %.8f"%(tstat, pvalue))

"""### 상관관계분석"""

wine.corr()

sns.heatmap(wine.corr(), annot=True, cmap='coolwarm')

#너무 상관관계가 높은 아이들끼리는 같이 쓰지말고 둘중 하나를 쓰는게 좋음 공분산도가 높아짐

wine.corrwith(wine.quality).sort_values()

"""### 산점도 pairplot
- 모든 회귀문제는 산점도가 있어야 파악이 용이
- 하지만 모든 점을 산점도를 그리면   성능문제발생,   그래서 샘플링을 하여 산점도로 변수간 관계를 만듦
"""

def sampling(data, n=300):         #와인 샘플링
    return data.loc[np.random.choice(data.index, size=n, replace=False)]        #choice  인덱스 골라냄,  replace 중복미허용F

red = wine[wine.type ==0]
white = wine[wine.type ==1]

red_sample = sampling(red)
print(red_sample.shape)
red_sample.head()

white_sample = sampling(white)
print(white_sample.shape)
white_sample.head()

wine_sample = pd.concat([red_sample, white_sample])

sns.pairplot(wine_sample, hue='type', kind='reg',
            vars=['quality', 'alcohol', 'density', 'volatile acidity', 'residual sugar'])     #pairplot 회귀문제에서 꼭 필요***



"""### .
- 선형회귀모형  만들기
"""

wine.columns = wine.columns.str.replace(' ','_')

wine.columns

features = ['fixed_acidity', 'volatile_acidity', 'residual_sugar',
       'chlorides', 'free_sulfur_dioxide', 'density',
       'pH', 'sulphates', 'alcohol', 'type']                              #예측모델을 만들기 위해 핏쳐 셀렉션을 함

features_B = ['volatile_acidity', 'residual_sugar','density','alcohol', 'type']

features_C = ['volatile_acidity', 'residual_sugar','density','alcohol', 'type', 'density']

features_D = ['volatile_acidity', 'residual_sugar','density','alcohol', 'type', 'density','fixed_acidity']

features_E = ['volatile_acidity', 'residual_sugar','density','alcohol', 'type', 'density','fixed_acidity', 'chlorides']

features_F = ['volatile_acidity', 'residual_sugar','density','alcohol', 'type', 'density','fixed_acidity', 'chlorides', 'free_sulfur_dioxide']

features_G = ['volatile_acidity', 'residual_sugar','density','alcohol', 'type', 'density','fixed_acidity', 'chlorides', 'free_sulfur_dioxide', 'pH']

features_H = ['volatile_acidity', 'residual_sugar','density','alcohol', 'type', 'density','fixed_acidity', 'chlorides', 'free_sulfur_dioxide', 'pH', 'sulphates']

y = wine.quality
X = wine[wine.columns.difference(['quality', 'type'])]
X_standard = (X - X.mean())/ X.std()    #표준화 : 단위기준을 맞춤
wine_standard = pd.concat([X_standard, wine.quality, wine.type], axis=1)

X = wine_standard[features]
X_B = wine_standard[features_B]
X_C = wine_standard[features_C]
X_D = wine_standard[features_D]
X_E = wine_standard[features_E]
X_F = wine_standard[features_F]
X_G = wine_standard[features_G]
X_H = wine_standard[features_H]

"""# 훈련데이터셋과 테스트데이터셋으로 나누자
### 회귀분석의 경우는.... 훈련데이터셋과 테스트 데이터 셋을 나누어야 함
"""

from sklearn.model_selection import train_test_split                   #머신러닝 라이브러리는 사이킷런에 있음

X_train, X_test, y_train, y_test = train_test_split(X, wine.quality, train_size=0.8, random_state=31)

X_B_train, X_B_test, y_B_train, y_B_test = train_test_split(X_B, wine.quality, train_size=0.8, random_state=31)
X_C_train, X_C_test, y_C_train, y_C_test = train_test_split(X_C, wine.quality, train_size=0.8, random_state=31)
X_D_train, X_D_test, y_D_train, y_D_test = train_test_split(X_D, wine.quality, train_size=0.8, random_state=31)
X_E_train, X_E_test, y_E_train, y_E_test = train_test_split(X_E, wine.quality, train_size=0.8, random_state=31)
X_F_train, X_F_test, y_F_train, y_F_test = train_test_split(X_F, wine.quality, train_size=0.8, random_state=31)
X_G_train, X_G_test, y_G_train, y_G_test = train_test_split(X_G, wine.quality, train_size=0.8, random_state=31)
X_H_train, X_H_test, y_H_train, y_H_test = train_test_split(X_H, wine.quality, train_size=0.8, random_state=31)



print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

print(X_B_train.shape)
print(X_B_test.shape)
print(y_B_train.shape)
print(y_B_test.shape)

# 모델정의

from sklearn.linear_model import LinearRegression

model = LinearRegression()

model_B = LinearRegression()

model.fit(X_train, y_train)

model_B.fit(X_B_train, y_B_train)

prediction = model.predict(X_test)

prediction_B = model_B.predict(X_B_test)

prediction

prediction_B

"""### 모델평가"""

from sklearn import metrics

metrics.mean_absolute_error(y_test, prediction)

metrics.mean_absolute_error(y_B_test, prediction_B)

metrics.mean_squared_error(y_test, prediction)                 #  오류율이 낮아야 좋은 모델

metrics.mean_squared_error(y_B_test, prediction_B)

"""### 하단 개인작업구간

### 숫자를 떨어뜨려라  (과제)
- 1. 그전에 핏쳐의 셀렉션과 조합, 핏처 엔지니어링... 등
- 2. 하이퍼 파라메터 수정해가며 수정
- 3. 랜덤포레스트 이용.

> **metrics.mean_squared_error(y_test, prediction)   오류율 낮추기
"""

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor

test_model = RandomForestRegressor(n_estimators = 300, 
                                  max_depth = 10, 
                                  max_features =0.9,
                                  random_state=31)

cross_val_score(test_model, X_train, y_train, scoring= 'neg_mean_squared_error', cv=10)                 #'neg_mean_squared_error'

cross_val_score(test_model, X_train, y_train, scoring= 'neg_mean_squared_error', cv=10).mean()

hyper_result =[]

max_depth = [3,5,7,9,11,13]
max_feature = [0.7,0.8,0.9,1.0]


for a in max_depth:
    for b in max_feature:
        model = RandomForestRegressor(n_estimators = 300, 
                                          max_depth = a, 
                                          max_features =b,
                                          min_samples_split=2,
                                          min_samples_leaf=1,
                                          random_state=31)

        score = cross_val_score(model, X_train, y_train, scoring= 'neg_mean_squared_error', cv=10).mean()

        hyper_result.append({'점수' : score,
                             '피쳐모델' : "basic",
                             'MAX_depth': a,
                             'MAX_features': b})
              

        print("현재 스코어는 {0:.5f}".format(score))

pd.DataFrame(hyper_result).sort_values(by='점수', ascending=False)

hyper_result_B =[]

max_depth = [3,5,7,9,11,13]
max_feature = [0.7,0.8,0.9,1.0]


for a in max_depth:
    for b in max_feature:
        model_B = RandomForestRegressor(n_estimators = 500, 
                                          max_depth = a, 
                                          max_features =b,
                                          min_samples_split=2,
                                          min_samples_leaf=1,
                                          random_state=31)

        score_B = cross_val_score(model_B, X_B_train, y_B_train, scoring= 'neg_mean_squared_error', cv=5).mean()

        hyper_result_B.append({'점수' : score_B,
                             '피쳐모델' : "B",
                             'MAX_depth': a,
                             'MAX_features': b})
              

        print("현재 스코어는 {0:.5f}".format(score_B))

pd.DataFrame(hyper_result_B).sort_values(by='점수', ascending=False)



hyper_result_C =[]



max_depth = [3,5,7,9,11,13]
max_feature = [0.7,0.8,0.9,1.0]


for a in max_depth:
    for b in max_feature:

        model_C = RandomForestRegressor(n_estimators = 500, 
                                      max_depth = a, 
                                      max_features =b,
                                      min_samples_split=2,
                                      min_samples_leaf=1,
                                      random_state=31)

        
        score_C = cross_val_score(model_C, X_C_train, y_C_train, scoring= 'neg_mean_squared_error', cv=5).mean()


        hyper_result_C.append({'점수' : score_C,
                               '피쳐모델' : "C",
                               'MAX_depth': a,
                               'MAX_features': b})      
        

        print("현재 스코어는 {0:.5f}".format(score_C))

pd.DataFrame(hyper_result_C).sort_values(by='점수', ascending=False)

hyper_result_D =[]



max_depth = [3,5,7,9,11,13]
max_feature = [0.7,0.8,0.9,1.0]


for a in max_depth:
    for b in max_feature:

        model_D = RandomForestRegressor(n_estimators = 500, 
                                      max_depth = a, 
                                      max_features =b,
                                      min_samples_split=2,
                                      min_samples_leaf=1,
                                      random_state=31)

        
        score_D = cross_val_score(model_D, X_D_train, y_D_train, scoring= 'neg_mean_squared_error', cv=5).mean()


        hyper_result_D.append({'점수' : score_D,
                               '피쳐모델' : "D",
                               'MAX_depth': a,
                               'MAX_features': b})      
        

        print("현재 스코어는 {0:.5f}".format(score_D))

pd.DataFrame(hyper_result_D).sort_values(by='점수', ascending=False)

hyper_result_E =[]



max_depth = [3,5,7,9,11,13]
max_feature = [0.7,0.8,0.9,1.0]


for a in max_depth:
    for b in max_feature:

        model_E = RandomForestRegressor(n_estimators = 500, 
                                      max_depth = a, 
                                      max_features =b,
                                      min_samples_split=2,
                                      min_samples_leaf=1,
                                      random_state=31)

        
        score_E = cross_val_score(model_E, X_E_train, y_E_train, scoring= 'neg_mean_squared_error', cv=5).mean()


        hyper_result_E.append({'점수' : score_E,
                               '피쳐모델' : "E",
                               'MAX_depth': a,
                               'MAX_features': b})      
        

        print("현재 스코어는 {0:.5f}".format(score_E))

pd.DataFrame(hyper_result_E).sort_values(by='점수', ascending=False)

hyper_result_F =[]



max_depth = [3,5,7,9,11,13]
max_feature = [0.7,0.8,0.9,1.0]


for a in max_depth:
    for b in max_feature:

        model_F = RandomForestRegressor(n_estimators = 500, 
                                      max_depth = a, 
                                      max_features =b,
                                      min_samples_split=2,
                                      min_samples_leaf=1,
                                      random_state=31)

        
        score_F = cross_val_score(model_F, X_F_train, y_F_train, scoring= 'neg_mean_squared_error', cv=5).mean()


        hyper_result_F.append({'점수' : score_F,
                               '피쳐모델' : "F",
                               'MAX_depth': a,
                               'MAX_features': b})      
        

        print("현재 스코어는 {0:.5f}".format(score_F))

pd.DataFrame(hyper_result_F).sort_values(by='점수', ascending=False)

hyper_result_G =[]



max_depth = [3,5,7,9,11,13]
max_feature = [0.7,0.8,0.9,1.0]


for a in max_depth:
    for b in max_feature:

        model_G = RandomForestRegressor(n_estimators = 500, 
                                      max_depth = a, 
                                      max_features =b,
                                      min_samples_split=2,
                                      min_samples_leaf=1,
                                      random_state=31)

        
        score_G = cross_val_score(model_G, X_G_train, y_G_train, scoring= 'neg_mean_squared_error', cv=5).mean()


        hyper_result_G.append({'점수' : score_G,
                               '피쳐모델' : "G",
                               'MAX_depth': a,
                               'MAX_features': b})      
        

        print("현재 스코어는 {0:.5f}".format(score_G))

pd.DataFrame(hyper_result_G).sort_values(by='점수', ascending=False)

hyper_result_H =[]



max_depth = [3,5,7,9,11,13]
max_feature = [0.7,0.8,0.9,1.0]


for a in max_depth:
    for b in max_feature:

        model_H = RandomForestRegressor(n_estimators = 500, 
                                      max_depth = a, 
                                      max_features =b,
                                      min_samples_split=2,
                                      min_samples_leaf=1,
                                      random_state=31)

        
        score_H = cross_val_score(model_H, X_H_train, y_H_train, scoring= 'neg_mean_squared_error', cv=5).mean()


        hyper_result_H.append({'점수' : score_H,
                               '피쳐모델' : "H",
                               'MAX_depth': a,
                               'MAX_features': b})      
        

        print("현재 스코어는 {0:.5f}".format(score_H))

pd.DataFrame(hyper_result_H).sort_values(by='점수', ascending=False)

print(pd.DataFrame(hyper_result)['점수'].max())
print(pd.DataFrame(hyper_result_B)['점수'].max())
print(pd.DataFrame(hyper_result_C)['점수'].max())
print(pd.DataFrame(hyper_result_D)['점수'].max())
print(pd.DataFrame(hyper_result_E)['점수'].max())
print(pd.DataFrame(hyper_result_F)['점수'].max())
print(pd.DataFrame(hyper_result_G)['점수'].max())
print(pd.DataFrame(hyper_result_H)['점수'].max())

# 최종 모델정의

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor

Finalmodel = RandomForestRegressor()
Finalmodel_B = RandomForestRegressor()
Finalmodel_C = RandomForestRegressor()
Finalmodel_D = RandomForestRegressor()
Finalmodel_E = RandomForestRegressor()
Finalmodel_F = RandomForestRegressor()
Finalmodel_G = RandomForestRegressor()
Finalmodel_H = RandomForestRegressor()

Finalmodel.fit(X_train, y_train)
Finalmodel_B.fit(X_B_train, y_B_train)
Finalmodel_C.fit(X_C_train, y_C_train)
Finalmodel_D.fit(X_D_train, y_D_train)
Finalmodel_E.fit(X_E_train, y_E_train)
Finalmodel_F.fit(X_F_train, y_F_train)
Finalmodel_G.fit(X_G_train, y_G_train)
Finalmodel_H.fit(X_H_train, y_H_train)

prediction = Finalmodel.predict(X_test)

prediction_B = Finalmodel_B.predict(X_B_test)
prediction_C = Finalmodel_C.predict(X_C_test)
prediction_D = Finalmodel_D.predict(X_D_test)
prediction_E = Finalmodel_E.predict(X_E_test)
prediction_F = Finalmodel_F.predict(X_F_test)
prediction_G = Finalmodel_G.predict(X_G_test)
prediction_H = Finalmodel_H.predict(X_H_test)

prediction

prediction_B

prediction_C

prediction_D

prediction_E

prediction_F

prediction_G

prediction_H

"""### 모델평가"""

from sklearn import metrics

metrics.mean_squared_error(y_test, prediction)                 #  오류율이 낮아야 좋은 모델

metrics.mean_squared_error(y_B_test, prediction_B)

metrics.mean_squared_error(y_B_test, prediction_B)

metrics.mean_squared_error(y_C_test, prediction_C)

metrics.mean_squared_error(y_D_test, prediction_D)

metrics.mean_squared_error(y_E_test, prediction_E)

metrics.mean_squared_error(y_F_test, prediction_F)

metrics.mean_squared_error(y_G_test, prediction_G)

metrics.mean_squared_error(y_H_test, prediction_H)          # 가장 낮은 오류율로 H모델 사용이 현재로써는 가장 유효함

"""### 상단 개인작업구간"""

formula = 'quality ~ alcohol + density + residual_sugar + type + volatile_acidity + chlorides'   #이꼴 표시이나 물결로 함

formula_all = 'quality ~ fixed_acidity + volatile_acidity + residual_sugar + chlorides + free_sulfur_dioxide + total_sulfur_dioxide + density + pH + sulphates + alcohol + type'

#코드를 거쳐 한 번 전체 확인 후(pvalue등...)  불필요한 증명력을 가진사항은 지워 봄

from statsmodels.formula.api import ols       #statsmodel:   모든 통계 공식이 들어있음.   ols:최소제곱법

formula_model = ols(formula, data=wine).fit()          #

formula_model.summary()                                #R-squared는  설명력을 갖추었 다는 것

#density  -33에 대한 확인
wine.head()
#0.9978    이렇게 단위차이의 격차가 너무 차이남. 그래서 단위기준을 맞추어 주어야 함. 표준화

"""### ** 독립변수의 표준화
- 종속변수는 절대로 표준화를 하면 안됨
"""

formula_model = ols(formula, data=wine_standard).fit()

formula_model.summary()

formula_all_model = ols(formula_all, data=wine_standard).fit()

formula_all_model.summary()